{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "oxiZ42B4SwQ-"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import time\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from helper import test_prediction, test_generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "x5znxQhLSwRC"
      },
      "outputs": [],
      "source": [
        "# load all that we need\n",
        "\n",
        "dataset = np.load('data/wiki.train.npy', allow_pickle=True)\n",
        "devset = np.load('data/wiki.valid.npy', allow_pickle=True)\n",
        "inference_pred = np.load('inference/prediction.npz')  # dev\n",
        "inference_gen = np.load('inference/generation.npy')  # dev\n",
        "inference_pred_test = np.load('inference/prediction_test.npz')  # test\n",
        "inference_gen_test = np.load('inference/generation_test.npy')  # test\n",
        "vocab = np.load('data/vocab.npy')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eCzNynwGYlJ0",
        "outputId": "5b0ef7c6-ff0b-48e8-8016-b451b3c330e1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "33278"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M-GQuKGEnBB_",
        "outputId": "cb536c4e-a817-4bcd-c1e0-ec2c4d50a0bd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(579,)\n",
            "(3803,)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "print(dataset.shape) #579 articles\n",
        "print(dataset[0].shape) #no. of words in articles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "OZNrJ8XvSwRF"
      },
      "outputs": [],
      "source": [
        "\n",
        "class LanguageModelDataLoader(DataLoader):\n",
        "    def __init__(self, dataset, batch_size, shuffle=True):\n",
        "        \n",
        "        self.dataset = dataset\n",
        "        self.batch_size = batch_size\n",
        "        self.shuffle = shuffle\n",
        "        \n",
        "        self.seq = 60\n",
        "\n",
        "    def __iter__(self):\n",
        "\n",
        "        def get_seq_len(seq_l):\n",
        "            if np.random.random() < 0.95:\n",
        "                seq_len = seq_l\n",
        "            else:\n",
        "                seq_len = seq_l/2\n",
        "            seq_len = round(np.random.normal(seq_len, 5))\n",
        "            return seq_len\n",
        "\n",
        "        \n",
        "        if self.shuffle == True:\n",
        "            np.random.shuffle(self.dataset)\n",
        "\n",
        "        \n",
        "        new_shuffled_conncat_dataset = np.concatenate(self.dataset)\n",
        "        print(new_shuffled_conncat_dataset)\n",
        "\n",
        "        self.no_of_batches = (len(new_shuffled_conncat_dataset)-1)//self.batch_size \n",
        "\n",
        "\n",
        "\n",
        "        no_of_elements = self.batch_size *self.seq \n",
        "\n",
        "        \n",
        "        \n",
        "        for i in range(self.no_of_batches):\n",
        "          no_of_elements = self.batch_size *self.seq \n",
        "\n",
        "          try:\n",
        "            x = new_shuffled_conncat_dataset[i*no_of_elements:i*no_of_elements+no_of_elements]\n",
        "            y = new_shuffled_conncat_dataset[(i*no_of_elements)+1:i*no_of_elements+no_of_elements+1]# X-> 2dim\n",
        "            \n",
        "            x = np.array(x).reshape(self.batch_size,self.seq)\n",
        "            y = np.array(y).reshape(self.batch_size,self.seq)\n",
        "\n",
        "            yield (torch.LongTensor(x),torch.LongTensor(y))\n",
        "          except:\n",
        "            pass\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KieWV1ZdnBCC",
        "outputId": "d2d3da7a-d9e6-4f91-f78d-b52d1b75fe23"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Exception ignored in: <generator object LanguageModelDataLoader.__iter__ at 0x7efb6c783580>\n",
            "Traceback (most recent call last):\n",
            "  File \"<ipython-input-25-a9eed6438260>\", line 14, in <cell line: 3>\n",
            "RuntimeError: generator ignored GeneratorExit\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ 1420  3170 12031 ... 17323    79  1417]\n",
            "X: \n",
            "torch.Size([64, 60])\n",
            "tensor([ 1420,  3170, 12031,    72, 13276, 10073,    73,  1420,  1417,     1,\n",
            "         3170, 12031,     1, 23592, 31353, 29496, 20398, 25821, 31353, 31422,\n",
            "        29294, 25821, 31353,  1822, 17716, 31244, 29456, 13276, 10073,    76,\n",
            "        15340, 31353, 29649,    64, 21511,  1424, 21107, 20398, 26120,    79,\n",
            "        14444, 16802,  2997,  5698,    76, 15340, 19338, 16802,  3116, 31353,\n",
            "        13864, 12489, 15340,  5584, 18510,  7565, 14242,    76, 31353, 20398])\n",
            "['=', 'Business', 'School', '(', 'The', 'Office', ')', '=', '<eol>', '\"', 'Business', 'School', '\"', 'is', 'the', 'seventeenth', 'episode', 'of', 'the', 'third', 'season', 'of', 'the', 'American', 'comedy', 'television', 'series', 'The', 'Office', ',', 'and', 'the', 'show', \"'s\", 'forty', '@-@', 'fifth', 'episode', 'overall', '.', 'Written', 'by', 'Brent', 'Forrester', ',', 'and', 'directed', 'by', 'Buffy', 'the', 'Vampire', 'Slayer', 'and', 'Firefly', 'creator', 'Joss', 'Whedon', ',', 'the', 'episode']\n",
            "Y: \n",
            "torch.Size([64, 60])\n",
            "tensor([ 3170, 12031,    72, 13276, 10073,    73,  1420,  1417,     1,  3170,\n",
            "        12031,     1, 23592, 31353, 29496, 20398, 25821, 31353, 31422, 29294,\n",
            "        25821, 31353,  1822, 17716, 31244, 29456, 13276, 10073,    76, 15340,\n",
            "        31353, 29649,    64, 21511,  1424, 21107, 20398, 26120,    79, 14444,\n",
            "        16802,  2997,  5698,    76, 15340, 19338, 16802,  3116, 31353, 13864,\n",
            "        12489, 15340,  5584, 18510,  7565, 14242,    76, 31353, 20398, 15109])\n",
            "['Business', 'School', '(', 'The', 'Office', ')', '=', '<eol>', '\"', 'Business', 'School', '\"', 'is', 'the', 'seventeenth', 'episode', 'of', 'the', 'third', 'season', 'of', 'the', 'American', 'comedy', 'television', 'series', 'The', 'Office', ',', 'and', 'the', 'show', \"'s\", 'forty', '@-@', 'fifth', 'episode', 'overall', '.', 'Written', 'by', 'Brent', 'Forrester', ',', 'and', 'directed', 'by', 'Buffy', 'the', 'Vampire', 'Slayer', 'and', 'Firefly', 'creator', 'Joss', 'Whedon', ',', 'the', 'episode', 'aired']\n"
          ]
        }
      ],
      "source": [
        "DL= LanguageModelDataLoader(dataset=dataset, batch_size=64, shuffle=True)\n",
        "for data in DL:\n",
        "    x,y = data\n",
        "    print('X: ')\n",
        "    print(x.shape)\n",
        "    print(x[0,:])\n",
        "    print([vocab[i] for i in x[0,:]])\n",
        "    print('Y: ')\n",
        "    print(x.shape)\n",
        "    print(y[0,:])\n",
        "    print([vocab[i] for i in y[0,:]])\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "0AAtI_40nBCE"
      },
      "outputs": [],
      "source": [
        "class LockedDropout(nn.Module):\n",
        "\n",
        "\n",
        "    def __init__(self, p=0.5):\n",
        "        self.p = p\n",
        "        super().__init__()\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x (:class:`torch.FloatTensor` [batch size, sequence length, rnn hidden size]): Input to\n",
        "                apply dropout too.\n",
        "        \"\"\"\n",
        "        if not self.training or not self.p:\n",
        "            return x\n",
        "        x = x.clone()\n",
        "        mask = x.new_empty(x.size(0),1, x.size(2), requires_grad=False).bernoulli_(1 - self.p)\n",
        "        mask = mask.div_(1 - self.p)\n",
        "        mask = mask.expand_as(x)\n",
        "        return x * mask\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zt-7YsTYSwRI"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "class LanguageModel(nn.Module):\n",
        "    def __init__(self, vocab_size):\n",
        "        super(LanguageModel, self).__init__()\n",
        "        \n",
        "        self.embedding = nn.Embedding(num_embeddings = vocab_size, embedding_dim = 512)\n",
        "        \n",
        "        self.lstm = nn.LSTM(hidden_size = 512, batch_first = True, num_layers =1, input_size = 512)\n",
        "\n",
        "        self.lock_drop = LockedDropout(p=0.3)\n",
        "\n",
        "        self.classification = nn.Linear(512, vocab_size)\n",
        "\n",
        "    def forward(self, x, h = None):\n",
        "\n",
        "        embedding1 = self.embedding(x)\n",
        "                \n",
        "        \n",
        "        if h == None:\n",
        "            embedding1, out2 = self.lstm(embedding1)\n",
        "        else:\n",
        "            embedding1, out2 = self.lstm(embedding1, h)\n",
        "        \n",
        "        x = self.lock_drop(embedding1)\n",
        "\n",
        "        embedding2, out2 = self.lstm(x, out2)\n",
        "\n",
        "        x = self.lock_drop(embedding2)\n",
        "\n",
        "        embedding3, out2 = self.lstm(x, out2)\n",
        "        \n",
        "        out = self.classification(embedding3)\n",
        "        \n",
        "        \n",
        "        return out, out2\n",
        "\n",
        "\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kIvZOIfjSwRK"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "class LanguageModelTrainer:\n",
        "    def __init__(self, model, loader, max_epochs=1, run_id='exp'):\n",
        "\n",
        "        self.model = model\n",
        "        self.loader = loader\n",
        "        self.train_losses = []\n",
        "        self.val_losses = []\n",
        "        self.predictions = []\n",
        "        self.predictions_test = []\n",
        "        self.generated_logits = []\n",
        "        self.generated = []\n",
        "        self.generated_logits_test = []\n",
        "        self.generated_test = []\n",
        "        self.epochs = 0\n",
        "        self.max_epochs = max_epochs\n",
        "        self.run_id = run_id\n",
        "        \n",
        "        # Define optimizer and criterion here\n",
        "        self.optimizer = torch.optim.Adam(model.parameters(),lr=0.002, weight_decay=3e-6)\n",
        "        self.criterion = nn.CrossEntropyLoss()\n",
        "        \n",
        "    def train(self):\n",
        "        self.model.train() # set to training mode\n",
        "        epoch_loss = 0\n",
        "        \n",
        "        for batch_num, (inputs, targets) in enumerate(self.loader):\n",
        "            epoch_loss += self.train_batch(inputs, targets)\n",
        "        epoch_loss = epoch_loss / (batch_num + 1)\n",
        "        self.epochs += 1\n",
        "        print('[TRAIN]  Epoch [%d/%d]   Loss: %.4f'\n",
        "                      % (self.epochs + 1, self.max_epochs, epoch_loss))\n",
        "        self.train_losses.append(epoch_loss)\n",
        "\n",
        "    def train_batch(self, inputs, targets):\n",
        "        # zero the parameter gradients\n",
        "\n",
        "        inputs = (inputs).cuda()\n",
        "        targets = (targets).cuda()\n",
        "\n",
        "        self.optimizer.zero_grad()\n",
        "        \n",
        "        outputs, h  = self.model(inputs)\n",
        "\n",
        "        outputs1 = outputs.view(-1, outputs.size(2))\n",
        "        targets1 = targets.view(-1)\n",
        "        \n",
        "        loss = self.criterion(outputs1, targets1) \n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "        \n",
        "        \n",
        "        return loss\n",
        " \n",
        "        \n",
        "\n",
        "    \n",
        "    def test(self):\n",
        "        # don't change these\n",
        "        self.model.eval() # set to eval mode\n",
        "\n",
        "        predictions = TestLanguageModel.prediction(inference_pred['inp'], self.model) # get predictions\n",
        "        self.predictions.append(predictions)\n",
        "\n",
        "        nll = test_prediction(predictions, inference_pred['out'])\n",
        "        self.val_losses.append(nll)\n",
        "\n",
        "        generated_logits = TestLanguageModel.generation(inference_gen, 10, self.model) # generated predictions for 10 words\n",
        "        self.generated_logits.append(generated_logits)\n",
        "        generated = test_generation(inference_gen, generated_logits, vocab)\n",
        "        self.generated.append(generated)\n",
        "\n",
        "        #TEST\n",
        "        # generate predictions for test data\n",
        "        predictions_test = TestLanguageModel.prediction(inference_pred_test['inp'], self.model) # get predictions\n",
        "        self.predictions_test.append(predictions_test)\n",
        "\n",
        "        generated_logits_test = TestLanguageModel.generation(inference_gen_test, 10, self.model)\n",
        "        generated_test = test_generation(inference_gen_test, generated_logits_test, vocab)\n",
        "        self.generated_logits_test.append(generated_logits_test)\n",
        "        self.generated_test.append(generated_test)\n",
        "        \n",
        "    \n",
        "        print('[VAL]  Epoch [%d/%d]   Loss: %.4f'\n",
        "                      % (self.epochs + 1, self.max_epochs, nll))\n",
        "        return nll\n",
        "\n",
        "\n",
        "    def save(self):\n",
        "        # don't change these\n",
        "        model_path = os.path.join('experiments', self.run_id, 'model-{}.pkl'.format(self.epochs))\n",
        "        torch.save({'state_dict': self.model.state_dict()},\n",
        "            model_path)\n",
        "        np.save(os.path.join('experiments', self.run_id, 'predictions-{}.npy'.format(self.epochs)), self.predictions[-1])\n",
        "        np.save(os.path.join('experiments', self.run_id, 'predictions-test-{}.npy'.format(self.epochs)), self.predictions_test[-1])\n",
        "        np.save(os.path.join('experiments', self.run_id, 'generated_logits-{}.npy'.format(self.epochs)), self.generated_logits[-1])\n",
        "        np.save(os.path.join('experiments', self.run_id, 'generated_logits-test-{}.npy'.format(self.epochs)), self.generated_logits_test[-1])\n",
        "        with open(os.path.join('experiments', self.run_id, 'generated-{}.txt'.format(self.epochs)), 'w') as fw:\n",
        "            fw.write(self.generated[-1])\n",
        "        with open(os.path.join('experiments', self.run_id, 'generated-{}-test.txt'.format(self.epochs)), 'w') as fw:\n",
        "            fw.write(self.generated_test[-1])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xPI7_kZRSwRN"
      },
      "outputs": [],
      "source": [
        "class TestLanguageModel:\n",
        "    def prediction(inp, model):\n",
        "\n",
        "        inp = torch.LongTensor(inp).cuda()\n",
        "        out, h= model(inp)\n",
        "\n",
        "        return out[:,-1].cpu().detach().numpy()\n",
        "        \n",
        "\n",
        "        \n",
        "    def generation(inp, forward, model):\n",
        "        \"\"\"\n",
        "            Generate a sequence of words given a starting sequence.\n",
        "            inp: Initial sequence of words (batch size, length)\n",
        "            forward: number of additional words to generate\n",
        "            generated words (batch size, forward)\n",
        "        \"\"\"    \n",
        "        \n",
        "        gen_words = []\n",
        "\n",
        "        inp = torch.LongTensor(inp).cuda()\n",
        "\n",
        "        \n",
        "        for i in range(forward):\n",
        "            if i ==0:\n",
        "                word, h = model(inp) \n",
        "                word = torch.argmax(word, dim=2)[:,-1].unsqueeze(1)\n",
        "                gen_words.append(word)\n",
        "\n",
        "            else:\n",
        "                \n",
        "                word, h = model(word, h)\n",
        "                word = torch.argmax(word, dim=2)[:,-1].unsqueeze(1)\n",
        "                gen_words.append(word)\n",
        "                \n",
        "                \n",
        "        return torch.cat(gen_words,dim=1).cpu().detach().numpy()\n",
        "        \n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TiUrjbEjSwRQ"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "NUM_EPOCHS = 25\n",
        "BATCH_SIZE = 64\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2HCVG5YISwRW",
        "outputId": "54a94616-c810-49b3-87ee-6e44f9f894ce"
      },
      "outputs": [],
      "source": [
        "run_id = str(int(time.time()))\n",
        "if not os.path.exists('./experiments'):\n",
        "    os.mkdir('./experiments')\n",
        "os.mkdir('./experiments/%s' % run_id)\n",
        "print(\"Saving models, predictions, and generated words to ./experiments/%s\" % run_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DbHH6zXTSwRa"
      },
      "outputs": [],
      "source": [
        "model = LanguageModel(len(vocab)).cuda()\n",
        "loader = LanguageModelDataLoader(dataset=dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "trainer = LanguageModelTrainer(model=model, loader=loader, max_epochs=NUM_EPOCHS, run_id=run_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7D8wTJkBSwRc",
        "outputId": "8450b3fd-383b-4727-f7b7-3c2ceeb8ecc1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[TRAIN]  Epoch [2/25]   Loss: 7.2971\n",
            "[VAL]  Epoch [2/25]   Loss: 6.0067\n",
            "Saving model, predictions and generated output for epoch 0 with NLL: 6.0067115\n",
            "[TRAIN]  Epoch [3/25]   Loss: 6.2330\n",
            "[VAL]  Epoch [3/25]   Loss: 5.2680\n",
            "Saving model, predictions and generated output for epoch 1 with NLL: 5.2679806\n",
            "[TRAIN]  Epoch [4/25]   Loss: 5.8317\n",
            "[VAL]  Epoch [4/25]   Loss: 5.0538\n",
            "Saving model, predictions and generated output for epoch 2 with NLL: 5.053804\n",
            "[TRAIN]  Epoch [5/25]   Loss: 5.5716\n",
            "[VAL]  Epoch [5/25]   Loss: 4.9065\n",
            "Saving model, predictions and generated output for epoch 3 with NLL: 4.906543\n",
            "[TRAIN]  Epoch [6/25]   Loss: 5.3619\n",
            "[VAL]  Epoch [6/25]   Loss: 4.7839\n",
            "Saving model, predictions and generated output for epoch 4 with NLL: 4.78391\n",
            "[TRAIN]  Epoch [7/25]   Loss: 5.1894\n",
            "[VAL]  Epoch [7/25]   Loss: 4.6533\n",
            "Saving model, predictions and generated output for epoch 5 with NLL: 4.6532893\n",
            "[TRAIN]  Epoch [8/25]   Loss: 5.0315\n",
            "[VAL]  Epoch [8/25]   Loss: 4.7075\n",
            "[TRAIN]  Epoch [9/25]   Loss: 4.8962\n",
            "[VAL]  Epoch [9/25]   Loss: 4.6241\n",
            "Saving model, predictions and generated output for epoch 7 with NLL: 4.624115\n",
            "[TRAIN]  Epoch [10/25]   Loss: 4.7799\n",
            "[VAL]  Epoch [10/25]   Loss: 4.6431\n",
            "[TRAIN]  Epoch [11/25]   Loss: 4.6686\n",
            "[VAL]  Epoch [11/25]   Loss: 4.6375\n",
            "[TRAIN]  Epoch [12/25]   Loss: 4.5701\n",
            "[VAL]  Epoch [12/25]   Loss: 4.5237\n",
            "Saving model, predictions and generated output for epoch 10 with NLL: 4.5237236\n",
            "[TRAIN]  Epoch [13/25]   Loss: 4.4854\n",
            "[VAL]  Epoch [13/25]   Loss: 4.5484\n",
            "[TRAIN]  Epoch [14/25]   Loss: 4.4046\n",
            "[VAL]  Epoch [14/25]   Loss: 4.5085\n",
            "Saving model, predictions and generated output for epoch 12 with NLL: 4.508494\n",
            "[TRAIN]  Epoch [15/25]   Loss: 4.3320\n",
            "[VAL]  Epoch [15/25]   Loss: 4.5259\n",
            "[TRAIN]  Epoch [16/25]   Loss: 4.2689\n",
            "[VAL]  Epoch [16/25]   Loss: 4.5564\n",
            "[TRAIN]  Epoch [17/25]   Loss: 4.2091\n",
            "[VAL]  Epoch [17/25]   Loss: 4.6333\n",
            "[TRAIN]  Epoch [18/25]   Loss: 4.1587\n",
            "[VAL]  Epoch [18/25]   Loss: 4.4882\n",
            "Saving model, predictions and generated output for epoch 16 with NLL: 4.4882402\n",
            "[TRAIN]  Epoch [19/25]   Loss: 4.1070\n",
            "[VAL]  Epoch [19/25]   Loss: 4.5139\n",
            "[TRAIN]  Epoch [20/25]   Loss: 4.0573\n",
            "[VAL]  Epoch [20/25]   Loss: 4.5262\n",
            "[TRAIN]  Epoch [21/25]   Loss: 4.0101\n",
            "[VAL]  Epoch [21/25]   Loss: 4.4977\n",
            "[TRAIN]  Epoch [22/25]   Loss: 3.9708\n",
            "[VAL]  Epoch [22/25]   Loss: 4.5261\n",
            "[TRAIN]  Epoch [23/25]   Loss: 3.9327\n",
            "[VAL]  Epoch [23/25]   Loss: 4.5449\n",
            "[TRAIN]  Epoch [24/25]   Loss: 3.8917\n",
            "[VAL]  Epoch [24/25]   Loss: 4.4954\n",
            "[TRAIN]  Epoch [25/25]   Loss: 3.8562\n",
            "[VAL]  Epoch [25/25]   Loss: 4.5403\n",
            "[TRAIN]  Epoch [26/25]   Loss: 3.8242\n",
            "[VAL]  Epoch [26/25]   Loss: 4.5239\n"
          ]
        }
      ],
      "source": [
        "best_nll = 1e30 \n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    trainer.train()\n",
        "    nll = trainer.test()\n",
        "    if nll < best_nll:\n",
        "        best_nll = nll\n",
        "        print(\"Saving model, predictions and generated output for epoch \"+str(epoch)+\" with NLL: \"+ str(best_nll))\n",
        "        trainer.save()\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iJUi1SGeWeEH"
      },
      "outputs": [],
      "source": [
        "train_losses = [i.cpu().item() for i in trainer.train_losses]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A_Foe2NNWmzC",
        "outputId": "1d3595df-5db0-4acf-8c5a-d8a36876f4f0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[7.297083854675293,\n",
              " 6.232972621917725,\n",
              " 5.831674575805664,\n",
              " 5.571553707122803,\n",
              " 5.361859321594238,\n",
              " 5.189429759979248,\n",
              " 5.031527042388916,\n",
              " 4.896214962005615,\n",
              " 4.779944896697998,\n",
              " 4.668581962585449,\n",
              " 4.5700578689575195,\n",
              " 4.485433578491211,\n",
              " 4.404641151428223,\n",
              " 4.331961154937744,\n",
              " 4.268939018249512,\n",
              " 4.209073543548584,\n",
              " 4.158695220947266,\n",
              " 4.107037544250488,\n",
              " 4.05729341506958,\n",
              " 4.010082244873047,\n",
              " 3.970776319503784,\n",
              " 3.9326627254486084,\n",
              " 3.891695022583008,\n",
              " 3.8562068939208984,\n",
              " 3.8241639137268066]"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 278
        },
        "id": "z2FmDqBCSwRf",
        "outputId": "f9cf3af0-2680-412a-aec1-795d78aa5b38"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "\n",
        "# plot training curves\n",
        "plt.figure()\n",
        "plt.plot(range(1, trainer.epochs + 1), train_losses, label='Training losses')\n",
        "plt.plot(range(1, trainer.epochs + 1), trainer.val_losses, label='Validation losses')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('NLL')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ipdbmqaGSwRh",
        "outputId": "c3c23edd-d9a7-4220-abca-009692c018ba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input | Output #0: while the group was en route , but only three were ultimately able to attack . None of them were | not to be a few of the two other than\n",
            "Input | Output #1: <unk> , where he remained on loan until 30 June 2010 . <eol> = = = Return to Manchester United | States for a <unk> of the <unk> in the <unk>\n",
            "Input | Output #2: 25 April 2013 , denoting shipments of 500 @,@ 000 copies . <eol> The song became One Direction 's fourth | single @-@ not the Republic of the network @-@ out\n",
            "Input | Output #3: , and Bruce R. ) one daughter ( Wendy J. <unk> ) and two grandchildren , died in <unk> , | and <unk> . <eol> In the <unk> of the <unk>\n",
            "Input | Output #4: Warrior were examples of this type . Because their armor was so heavy , they could only carry a single | @-@ <unk> . This , and the two other <unk>\n",
            "Input | Output #5: the embassy at 1 : 49 and landed on Guam at 2 : 23 ; twenty minutes later , Ambassador | <unk> of the two other than the <unk> in the\n",
            "Input | Output #6: <unk> , $ 96 million USD ) . Damage was heaviest in South Korea , notably where it moved ashore | . In the <unk> @-@ <unk> @-@ <unk> @-@ <unk>\n",
            "Input | Output #7: The <unk> were condemned as <unk> by <unk> , who saw the riots as hampering attempts to resolve the situation | . <unk> , and <unk> , and <unk> , and\n",
            "Input | Output #8: by a decision made by the War Office in mid @-@ 1941 , as it was considering the equipment to | be a new <unk> the <unk> of the <unk> in\n",
            "Input | Output #9: Division crossed the <unk> at a number of places and climbed the hills quietly toward the 9th Infantry river line | . <unk> , the <unk> , and <unk> and two\n",
            "Input | Output #10: = <eol> = = = French VIII . Corps ( Corps <unk> ) = = = <eol> On 6 November | 2016 in a <unk> of the <unk> of the Jin\n",
            "Input | Output #11: of the World from 9th Avenue \" . This is regarded as his most famous work . It is considered | a second @-@ in the <unk> in the <unk> ,\n",
            "Input | Output #12: — <unk> @-@ 10 , <unk> @-@ 12 , <unk> @-@ 16 , <unk> @-@ 17 — were all converted | to be a new <unk> a new <unk> a new\n",
            "Input | Output #13: And now he has . \" <eol> = = Family = = <eol> <unk> lived 37 of his years in | the <unk> of the <unk> in the <unk> in the\n",
            "Input | Output #14: Hell to which he has been condemned for <unk> . Eliot , in a letter to John <unk> dated 27 | March a <unk> of the <unk> of the <unk> in\n",
            "Input | Output #15: Luoyang area , fulfilling his duties in domestic affairs . <eol> In the autumn of <unk> , he met Li | <unk> of the Jin in the Jin in the Jin\n",
            "Input | Output #16: Power said they enjoyed Block Ball and its number of stages , but wondered how its eight <unk> of memory | are the <unk> . In the <unk> , and <unk>\n",
            "Input | Output #17: by Lloyd F. Lonergan . The cameraman was Jacques <unk> . <eol> = = Release and reception = = <eol> | In the <unk> , and <unk> @-@ <unk> in a\n",
            "Input | Output #18: alone , the Austrians lost more than half their reserve artillery park , 6 @,@ 000 ( out of 8 | @,@ <unk> with the <unk> of the <unk> in the\n",
            "Input | Output #19: while attacking a ship at <unk> in the Dutch East Indies ; the loss was compounded by the fact that | the <unk> of the <unk> of the <unk> in the\n",
            "Input | Output #20: first raised in 2007 by the member of parliament ( MP ) for <unk> . The gangsters may have run | in the <unk> in a new <unk> a new <unk>\n",
            "Input | Output #21: Species are also non @-@ spiny <unk> and includes both large trees with stout stems up to 30 metres ( | 66 . <unk> of the <unk> of the <unk> of\n",
            "Input | Output #22: \" : specific design issues with the building 's energy efficiency included the fact that the largest room in the | world @-@ <unk> <unk> and <unk> . It 's <unk>\n",
            "Input | Output #23: were reported to support over 300 @,@ 000 households in the Brazilian state of <unk> in 2005 , and in | the <unk> in the <unk> in the <unk> in the\n",
            "Input | Output #24: port . <unk> in Vietnam also warned for the potential of heavy rainfall due to the dissipating Tropical Depression <unk> | , the storm in the storm of the storm of\n",
            "Input | Output #25: T @-@ numbers in their tropical cyclone products . The following example is from discussion number 3 of Tropical Depression | Twenty @-@ a <unk> of the <unk> in the <unk>\n",
            "Input | Output #26: South Australia hosted the three @-@ game semi @-@ final series against the New South Wales <unk> . Both teams | have the team , and <unk> , and <unk> in\n",
            "Input | Output #27: Perth from contention and secured the last finals spot for the <unk> . <eol> = = = Statistical leaders = | = = = = = = = = = =\n",
            "Input | Output #28: deemed it an \" amazing pop song \" , lauding the group 's falsetto and its \" head @-@ <unk> | \" . \" <unk> , and <unk> , and <unk>\n",
            "Input | Output #29: , but began patrolling the English Channel after <unk> @-@ 6 pioneered a route past British <unk> nets and mines | . They had a new <unk> <unk> in the <unk>\n",
            "Input | Output #30: production executives to let him direct . He had already discussed the film with <unk> and Cohen , and felt | that was the production of the film in a new\n",
            "Input | Output #31: and Nick <unk> at Studio <unk> in Los Angeles , California , and was released on August 1 , 2006 | . In the album of the album , and <unk>\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# see generated output\n",
        "print (trainer.generated[-1]) # get last generated output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fm1huQ41XrYo"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
